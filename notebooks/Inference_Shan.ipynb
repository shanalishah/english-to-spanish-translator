{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f037bab9-59e6-40d9-bdbc-a1627e4400ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script (`Translator_Shan.ipynb`) loads the trained Transformer model along with the saved TextVectorization layers, and\n",
    "# allows the user to input English sentences and receive Spanish translations.\n",
    "# All components (model weights and vectorizers) are pre-trained and saved, so no additional training is required to run this script.\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "from keras.models import load_model\n",
    "from transformer import Transformer\n",
    "from keras.saving import register_keras_serializable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3040f80-881e-4d4a-b30f-3ba0893b2777",
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_keras_serializable()\n",
    "def custom_standardization(input_string):\n",
    "    strip_chars = string.punctuation + \"¿\"\n",
    "    strip_chars = strip_chars.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    return tf.strings.regex_replace(tf.strings.lower(input_string), f\"[{re.escape(strip_chars)}]\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae536ce8-6f0a-4bf8-971c-20af53b1129d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vectorizers\n",
    "source_vectorization = load_model(\"source_vectorizer.keras\")\n",
    "target_vectorization = load_model(\"target_vectorizer.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68b3ff35-347b-4f77-bd89-35274e387037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocab for decoding\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07de64f6-a331-4480-b867-19ab364eba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild model architecture and load weights\n",
    "vocab_size = 15000\n",
    "seq_length = 20\n",
    "model = Transformer(n_layers=4, d_emb=128, n_heads=8, d_ff=512, dropout_rate=0.1,\n",
    "                    src_vocab_size=vocab_size, tgt_vocab_size=vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1225a54-131d-4e04-a344-4d9239ac9d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'global_self_attention' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'encoder_layer' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'encoder' (of type Encoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'causal_self_attention' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'decoder_layer' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'decoder' (of type Decoder) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Dummy call to build model\n",
    "dummy_input = tf.constant([[1] * seq_length])\n",
    "model((dummy_input, dummy_input))\n",
    "model.load_weights(\"translation_transformer.weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d1db5e4-b17a-4fd8-b23b-d8aff05cc5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate function\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(seq_length):\n",
    "        tokenized_target_sentence = target_vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = model((tokenized_input_sentence, tokenized_target_sentence))\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence.replace(\"[start] \", \"\").replace(\" [end]\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "319f759f-59df-43bc-aab7-523ac469c96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter English sentence (or type 'exit'):  What is your name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish: cómo se llama tu nombre\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter English sentence (or type 'exit'):  I'm good, how are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish: soy bueno como tú eres\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter English sentence (or type 'exit'):  I have completed this project.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spanish: he terminado este proyecto\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter English sentence (or type 'exit'):  exit\n"
     ]
    }
   ],
   "source": [
    "# Interactive prompt\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        input_sentence = input(\"\\nEnter English sentence (or type 'exit'): \")\n",
    "        if input_sentence.lower() == \"exit\":\n",
    "            break\n",
    "        print(\"Spanish:\", decode_sequence(input_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fee166-c31a-4c3b-bd75-c2fb88934f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
